---
title: "Airport effects on U.S. County Unemployment Rates"
author: "Robby Powell"
date: "`r format(Sys.Date(), '%Y-%B-%d')`"
output: 
  html_document: 
    keep_md: yes
    toc: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(fig.width = 9, fig.height = 9, fig.path = "figures/")
```

# Introduction
This is my response to Ari Lamstein's R Shapefile [contest](http://www.arilamstein.com/blog/2016/07/12/announcing-r-shapefile-contest/). 

## Analysis Question
My Analysis consists of my efforts to answer the following questions:

  1. What is the effect of distance to the nearest airport on a county's unemployment rate?
  
  2. How does the traffic of the nearest airport affect unemployment rates?
  
## Analysis Plan
The Analysis plan is delineated below. 

1. Import and Clean Data.
2. Visualize each of the main data elements by year.
3. Visualize relationships between variables.
4. Visualize the airport locations on a U.S. Map with County boundaries.
5. Calculate Distances between county centroids and:
    a. the nearest airport
    b. the nearest airport in the Top 50 of average passenger movements
6. Visualize the distances in two maps.
7. Create a regression model and perform diagnostics.

## Data Sources
One of the rules of the contest is that the data are available for everyone to use. In order to meet this requirement, there are links for the data and shapefile locations below. 

  1. Airport Traffic: [Federal Aviation Administration (FAA)](http://www.faa.gov/airports/planning_capacity/passenger_allcargo_stats/passenger/)

  2. Airport Locations: [openflights.org](http://openflights.org/data.html)
  
  3. County Shapefile: [US Census Bureau](https://www.census.gov/geo/maps-data/data/tiger-line.html)
  
  4. Unemployment data: [Bureau of Labor Statistics (BLS)](http://download.bls.gov/pub/time.series/la/)
  
Where feasible, I have also included a series of scripts that may be used to download the exact files that I used for the analysis. These are located in the Appendicies along with the full analysis script for ease of implementation/testing by other analysts.
  
If it is not listed in this section, then it will be listed as either an R package, or it will be something that I create during the analysis and write-up processes.

# Setup
First, we need to set up our script for the analysis. In this case, we are setting up the main directory structure and the packages required for the analysis.

## Directory setup

In my case, I prefer to start the document off with setting up my main directories in variables. This allows me to use the `paste` and `paste0` functions in base R in order to ease the transfer of the scripts to other directories. 

If you change directories, then all you need to do is to alter the `main.dir` variable, and everything else will follow. This reduces the need for changing the directory in multiple locations throughout an analysis.

```{r directorySetup}
# Main directory
main.dir <- "~/spatial-analysis-in-r/"

# Script directory
script.dir <- paste0(main.dir, "scripts/")

# data directory
dat.dir <- paste0(main.dir, "data/")
```

## Package setup

The packages utilized in this analysis are below. Unless otherwise noted, they are available from CRAN.

The comments after the packages explain why I am using them.

```{r loadPackages, warning = FALSE, error = FALSE, message = FALSE}
# data import
library(openxlsx)

# data cleaning
library(dplyr)
library(magrittr)
library(stringr)

# data table display
library(Hmisc)
library(tables)

# general plotting
library(ggplot2)

# mapping
library(sp)
library(rgdal)
library(GISTools)

# distance calculation
library(geosphere)
```

# Data Import and Cleaning
In this section, I will import and clean the data.

## BLS Data
First, I will import the main data set, which is labeled `la.data.64.County`. This contains all of the unemployment numbers.

```{r blsDataPrep1}
# Import UE Data
fname <- paste0(dat.dir, "/bls/la.data.64.County")
bls.dat <- read.delim(fname, header = TRUE, stringsAsFactors = FALSE, 
                      sep = "\t")

# trim white space in series_id
bls.dat$series_id <- trimws(bls.dat$series_id)

# change value to numeric
bls.dat$value <- as.numeric(bls.dat$value)

head(bls.dat)
```

Upon examination, the missing values are for the years 2005 and 2006, which means that they are not of interest to this analysis.

After getting the basic preparation for the main dataset, it is time to import the crosswalks for easier identification.

```{r blsDataPrep2}
# Import la.measure
fname <- paste0(dat.dir, "/bls/la.measure")
bls.measure <- read.delim(fname, header = TRUE, stringsAsFactors = FALSE, 
                      sep = "\t")
```

After looking at this data, the variables are mis-labeled. These will be fixed below.

```{r blsDataPrep3}
# fix variables
bls.measure$measure_text <- bls.measure$measure_code
bls.measure$measure_code <- as.numeric(rownames(bls.measure))

bls.measure
```

Import and examine the period definitions.

```{r blsDataPrep4}
# la.period
fname <- paste0(dat.dir, "/bls/la.period")
bls.period <- read.delim(fname, header = TRUE, stringsAsFactors = FALSE, 
                      sep = "\t")

bls.period
```

Import series definitions, which contains the names of the counties.

```{r blsDataPrep6}
# la.period
fname <- paste0(dat.dir, "/bls/la.series")
bls.series <- read.table(fname, header = TRUE, stringsAsFactors = FALSE, 
                      sep = "\t")

# trim white space from series_id
bls.series$series_id <- trimws(bls.series$series_id)

head(bls.series)
```

Now that all of the necessary data elements are imported, it's time to do some cleaning on the BLS dataset. 

In this case, the data needs to be joined with the proper identifiers, and then the dataset will need to be reduced.

Because we are interested in the annual averages, we will filter for period `M13`. In addition, we will filter for the years 2011-2015. After we have joined the data, we will select only required variables.

```{r blsDataPrep7}
# filter for period M13
bls.dat <- bls.dat[bls.dat$period == "M13",]

# filter for year
bls.dat <- bls.dat[bls.dat$year %in% c(2011:2015),]
 
# Join data together via a left join.
bls.dat <- merge(bls.dat, bls.series,
                 by.x = "series_id", by.y = "series_id",
                 all.x = TRUE)

bls.dat <- merge(bls.dat, bls.measure,
                 by.x = "measure_code", by.y = "measure_code", all.x = TRUE)

# filter for unemployment rate
bls.dat <- bls.dat[bls.dat$measure_text == "unemployment rate", ]

# reduce dataset
bls.dat <- bls.dat[, c("series_id", "year", "value", 
                       "series_title", "measure_text")]

# Create county name and state variables for easier identification
str.pattern <- "[A-za-z.//]*\\s*[A-za-z.//]*\\s*[A-Za-z.//]*,"

bls.dat$county <- str_extract(bls.dat$series_title, str.pattern)

bls.dat$county <- trimws(gsub(",", "", bls.dat$county))

bls.dat$state <-  str_extract(bls.dat$series_title, "[A-Z]{2}")

# Extract FIPS Code
bls.dat$FIPS <- substr(bls.dat$series_id, 6, 10)

# Select columns
bls <- bls.dat[c("FIPS", "county", "state", "year", "value")]

# Change names
names(bls) <- c("FIPS", "county", "state", "year", "UE")
```

Here is the final BLS dataset.

```{r blsDataPrep8}
head(bls)
summary(bls)
```


## Airport Data
Here, we import and check the airport dataset. This dataset is really clean, so it will be utilized as-is.

```{r airportPrep}
fname <- paste0(dat.dir, "airports/airports.dat.txt")
airports <- read.csv(fname, header = TRUE, stringsAsFactors = FALSE)

summary(airports)
head(airports)
```

## FAA Airport Traffic Data
The FAA Airport Data consists of four Excel spreadsheets. These will be imported and combined before they are cleaned.

```{r faaPrep}
fname <- paste0(dat.dir, "faa/CY12AllEnplanements.xlsx")
faa.cy12 <- openxlsx::read.xlsx(fname)

fname <- paste0(dat.dir, "faa/cy13-all-enplanements.xlsx")
faa.cy13 <- openxlsx::read.xlsx(fname)

fname <- paste0(dat.dir, "faa/CY14-all-enplanements.xlsx")
faa.cy14 <- openxlsx::read.xlsx(fname, sheet = "data")

fname <- paste0(dat.dir, "faa/preliminary-cy15-all-enplanements.xlsx")
faa.cy15 <- openxlsx::read.xlsx(fname)

# Select Annual Data, Rename columns, and add year.
faa.cy11 <- faa.cy12[, c("ST", "Locid", "City", "Airport.Name", 
                         "CY.11.Enplanements")]
names(faa.cy11) <- c("state", "id", "city", "airport.name", "enplanements")
faa.cy11$year <- 2011

faa.cy12 <- faa.cy12[, c("ST", "Locid", "City", "Airport.Name", 
                         "CY.12.Enplanements")]
names(faa.cy12) <- c("state", "id", "city", "airport.name", "enplanements")
faa.cy12$year <- 2012

faa.cy13 <- faa.cy13[, c("ST", "Locid", "City", "Airport.Name",
                         "CY.13.Enplanements")]
names(faa.cy13) <- c("state", "id", "city", "airport.name", "enplanements")
faa.cy13$year <- 2013

faa.cy14 <- faa.cy14[, c("ST", "Locid", "City", "Airport.Name",
                         "CY.14.Enplanements")]
names(faa.cy14) <- c("state", "id", "city", "airport.name", "enplanements")
faa.cy14$year <- 2014

faa.cy15 <- faa.cy15[, c("ST", "Locid", "City", "Airport.Name",
                         "CY.15.Enplanements")]
names(faa.cy15) <- c("state", "id", "city", "airport.name", "enplanements")
faa.cy15$year <- 2015

# Combine
faa <- rbind(faa.cy11, faa.cy12, faa.cy13, faa.cy14, faa.cy15)

summary(faa)
head(faa)
```



## Shapefile Import
This first snippet of code shows how to import the Census Shapefile.

```{r shapefileImport}
dsn <- "/Users/Robby/spatial-analysis-in-r/data/census/"
layer <- "tl_2015_us_county"
county <- rgdal::readOGR(dsn = dsn, layer = layer)
```

Because the United States has such a large spread of territory and protectorates around the world, it is necessary to set limits for the maps. In this case, we will be using maps for the Continental U.S. (CONUS), Alaska, and Hawaii. The limits chosen for this exercise are listed below.

```{r plotLimitsCONUS}
# Continental US (CONUS)
conus.min.lat <- 25
conus.max.lat <- 50
conus.min.long <- -126
conus.max.long <- -65

conus.long.limits <- c(conus.min.long, conus.max.long)
conus.lat.limits <- c(conus.min.lat, conus.max.lat)
```

```{r plotLimitsHawaii}
# Hawaii
hawaii.min.lat <- 18
hawaii.max.lat <- 23
hawaii.min.long <- -161 
hawaii.max.long <- -154

hawaii.long.limits <- c(hawaii.min.long, hawaii.max.long)
hawaii.lat.limits <- c(hawaii.min.lat, hawaii.max.lat)
```

```{r plotLimitsAlaska}
# Alaska
alaska.min.lat <- 52
alaska.max.lat <- 72
alaska.min.long <- -177
alaska.max.long <- -129

alaska.long.limits <- c(alaska.min.long, alaska.max.long)
alaska.lat.limits <- c(alaska.min.lat, alaska.max.lat)
```

## Identify Counties associated with Airports
Now that we have all of the data available, it is time to identify the relationship between the airports and counties. In this effort, I will first overlay the airports over the counties, and then determine which county contains the airports.

An output dataset will be produced, which can then be combined with all of the data to produce the final dataset.

First, in order to get an idea of what things look like, we will need to plot the airports and the counties on the same map. In order to reduce the number of images, I will put the maps together in a single image.

```{r countyAirportIdentification1}
# filter airport coordinates based on FAA codes in faa.data
airports <- airports[airports$country == "United States",]

#filter for CONUS, AK, and HI airports for plotting purposes-only
airports2 <- 
  airports[(between(airports$latitude, conus.min.lat, conus.max.lat) &
           between(airports$longitude, conus.min.long, conus.max.long)) |
           (between(airports$latitude, hawaii.min.lat, hawaii.max.lat) &
           between(airports$longitude, hawaii.min.long, hawaii.max.long)) |
           (between(airports$latitude, alaska.min.lat, alaska.max.lat) &
           between(airports$longitude, alaska.min.long, alaska.max.long)),]

# Prep airport coordinates
coordinates(airports) <- c("longitude", "latitude")
proj4string(airports) <- proj4string(county)

coordinates(airports2) <- c("longitude", "latitude")
proj4string(airports2) <- proj4string(county)

# Set layout for image.
layout(matrix(c(1,1,1,1,
                1,1,1,1,
                2,2,3,3,
                2,2,3,3), ncol = 4, nrow = 4, byrow = TRUE),
       respect = TRUE)

# CONUS Plot (Plot 1)
plot(county, xlim = conus.long.limits, ylim = conus.lat.limits,
     main = "Plot of Airports and Counties", border = "light grey",
     lwd = 0.25)
points(airports2, xlim = conus.long.limits, ylim = conus.lat.limits,
       pch = 20, cex = 0.5)

# Hawaii Plot (Plot 2)
plot(county, xlim = hawaii.long.limits, ylim = hawaii.lat.limits,
     border = "light grey", lwd = 0.25)
points(airports2, xlim = hawaii.long.limits, ylim = hawaii.lat.limits,
       pch = 20)

# Alaska Plot (Plot 3)
plot(county, xlim = alaska.long.limits, ylim = alaska.lat.limits,
     border = "light grey", lwd = 0.25)
points(airports2, xlim = alaska.long.limits, ylim = alaska.lat.limits,
       pch = 20)
```

This snippet of code will determine which the county within which each airport resides. 

```{r countyAirportIdentification2}
# get county ID
airports$county.id <- 
  sp::over(airports, county)$GEOID

# look at results
head(airports)
```

## Calculate County Distance to the Nearest Airport
Now, it is time to calculate the distance of the county centroids to their nearest airport. In order to determine this quantity, we will utilize the Great Circle Formula, or, more specifically, the Haversine Formula to calculate the distances.

Thankfully, we do not have to create this formula from scratch. It is located in the `geosphere` package, which is available on CRAN. The results will be in meters. 

Due to the way the output is being used, there is no need to change units; however, because distance is easier for most to understand in miles, we will convert the resulting distances to miles after we have determined the minimum distance for the airports.

```{r distCalc}
# Set up output data frame
apt.dist.dat <- data.frame(county.id = county$GEOID,
                           stringsAsFactors = F)

# Calculate distance between each airport
# NOTE: Consider building function instead of looping.
for (apt in airports$airport.id) {
  # The id is created such that the numbers will not make R think it is
  # creating column number XXX vice just adding a new column.
  apt.id <- paste0("airport.", apt)
  
  # get coordinates for airports and counties
  apt.coords <- coordinates(airports)[airports$airport.id == apt,]
  county.coords <- coordinates(county)
  
  # calculate distance
  apt.dist.dat[, apt.id] <- distHaversine(apt.coords, county.coords)
}
```

Now that we have the distances calculated, we have to determine which airport is the closest.

To make things easier and faster, a function will be created to calculate the minimum distance for a set of columns (the airports). Then the `apply` function will be utilized in lieu of a loop to increase computational efficiency.

```{r distCalcMin}

# This function will return the minimum distance.
min.dist.fn <- function(temp) {

  # Select airport with smallest distance
  min.index <- which.min(temp)
  
  # Return distance
  min.dist <- temp[min.index]
  
  # Get airport ID
  apt.id <- names(temp)[min.index]
  
  # output data
  out <- cbind(min.dist, apt.id)
  return(out)
}

# Get counties
counties <- apt.dist.dat$county.id
# Get minimum distance
min.dist <- apply(apt.dist.dat[,-1], MARGIN = 1, FUN = min.dist.fn)

# transpose and convert from list
min.dist <- t(unlist(min.dist))

# create data frame
min.dist <- data.frame(min.dist, stringsAsFactors = F)

# give column names
names(min.dist) <- c("min.distance", "airport.id")

# convert min.dist to miles
min.dist$min.distance <- as.numeric(min.dist$min.distance)/1609.34

# Get airport ID number
min.dist$airport.id <- gsub("airport.", "", min.dist$airport.id)

# create final distance data set.
cnty.dist <- cbind(counties, min.dist)

cnty.dist$counties <- as.character(cnty.dist$counties)
# look at resulting data set
summary(cnty.dist)
head(cnty.dist)
```

## Create Final Data Set
Now that the data have been calculated and cleaned, it is time to put together the final analysis set.

Since the BLS data likely has most, if not all, counties available, we will start with that dataset. Then we will join with the distance to the nearest airport, airport ID, and then the current-year traffic at that airport. 

Where possible, the data will be joined by both county FIPS and year. Otherwise, the same values will be assumed for each year.

```{r finalAnalysisData}
# join BLS data and cnty.dist dat
analysis.dat <- merge(bls, cnty.dist,
                      by.x = "FIPS", by.y = "counties", 
                      all.x = TRUE)

# join airports with analysis.dat
analysis.dat <- merge(analysis.dat, 
                      airports[, c("airport.id", "iata.faa.code", "county.id")],
                      by.x = "airport.id", by.y = "airport.id", all.x = TRUE)

# join with faa data
analysis.dat <- merge(analysis.dat, faa[, c("id", "year", "enplanements")],
                      by.x = c("iata.faa.code", "year"),
                      by.y = c("id", "year"), all.x = TRUE)

# determine if airport located within county
analysis.dat$airport.in.cnty <- 
    ifelse(analysis.dat$FIPS == analysis.dat$county.id, 1, 0)


# select and reorder remaining variables
vars <- c("FIPS", "county", "state", "year", "UE", "min.distance", 
          "enplanements", "airport.in.cnty")

analysis.dat <- analysis.dat[, vars]

# look at data
dim(analysis.dat)
summary(analysis.dat)
head(analysis.dat)

# show complete cases
dim(analysis.dat[complete.cases(analysis.dat),])
summary(analysis.dat[complete.cases(analysis.dat),])
head(analysis.dat[complete.cases(analysis.dat),])

```

# Exploratory Analysis

# Relationship between distance to airport and unemployment rate

## Training and Testing Datasets

## Training the Model

## Testing the Model

## Analyzing the Model

# Discussion

# Conclusions

# Computer Environment
The computer used for the analysis was a Late 2013 15" MacBook Pro, with 16 GB 1600 MHz DDR3 RAM and a 2.6 GHz Intel Core i7 Processor.

```{r computerInfo}
sessionInfo()
```

# Appendices
This section will show the full code of the analysis scripts.

## Appendix A: Complete Analysis
```{r appendixA, eval = FALSE}
# PLACE HOLDER FOR CODE HERE

# NEED TO SPIN RMD DOCUMENT AND DELETE APPENDIX CODE BEFORE PASTING

```

## Appendix B: Airport Location from openflight.org
```{r appendixB, eval = FALSE}
# File url
file <- "https://raw.githubusercontent.com/jpatokal/openflights/master/data/airports.dat"

# destination
dest <- "~/spatial-analysis-in-r/data/airports/airports.dat.txt"

# download file
download.file(file, dest)

# Check downloaded file
airports <- read.csv(dest, header = FALSE)

# name columns
names(airports) <- c("airport.id", "name", "city", "country", "iata.faa.code",
                     "icao.code", "latitude", "longitude", "altitude",
                     "UTC.offset", "DST", "time.zone.olson")

# save file with column names
  write.csv(airports, dest, row.names = FALSE)
```

## Appendix C: County Shapefile from Census Bureau
```{r appendixC, eval = FALSE}
# County shapefile url
url <- "ftp://ftp2.census.gov/geo/tiger/TIGER2015/COUNTY/tl_2015_us_county.zip"

# path
path <- "~/spatial-analysis-in-r/data/census/"

# destination
dest <- paste0(path, "tl_2015_us_county.zip")

# download file
download.file(url, dest)

# unzip file
unzip(dest, exdir = path, junkpaths = TRUE)
```

## Appendix D: BLS Data
```{r appendixD, eval = FALSE}

# Set main part of url for downloading the files
main.url <- "http://download.bls.gov/pub/time.series/la/"

# Set main directory for download
download.dir <- "~/spatial-analysis-in-r/data/bls/"

# List of files to download
file.list <- c("la.area", "la.area_type", "la.data.64.County", "la.measure",
               "la.period", "la.series", "la.state_region_division")

# combine download urls
download.urls <- unlist(lapply(main.url, FUN = "paste0", file.list))

# combine for file locations
download.locations <- unlist(lapply(download.dir, FUN = "paste0", file.list))

# download files
for (file in 1:length(file.list)) {
  if (!(file.list[file] %in% dir(download.dir))) {
    download.file(url = download.urls[file], destfile = download.locations[file])
  }
}
```